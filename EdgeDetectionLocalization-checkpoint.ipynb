{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bb170a",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2ee256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "from numpy import asarray\n",
    "from PIL import ImageEnhance, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d9c45",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "## This code provides an implementation of edge detection using OpenCV and various filters including:\n",
    "    Sobel, Laplacian, HBF, SHBF, Prewitt and Scharr.\n",
    "\n",
    "## Usage\n",
    "To use this function, you need to have the OpenCV, numpy, and PIL libraries installed. You can call the EdgeDetection function by providing the path to the input image as the first argument and a Boolean value (True or False) as the second argument to specify whether to use the Canny filter or not. The output will be a window displaying the edge detected image, The output of the EdgeDetection function is a window displaying the edge detected image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504d03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EdgeDetection(imagePath, Canny):\n",
    "    img = cv2.imread(imagePath)\n",
    "    dst = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)\n",
    "    gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "    sobel_filter_vertical=np.array([[-1,0,+1],\n",
    "                 [-2,0,+2],\n",
    "                [-1,0,+1]])\n",
    "    sobel_filter_horizontal=np.array([[1,2,+1],\n",
    "                 [0,0,0],\n",
    "                [-1,-2,-1]])\n",
    "    sharp_image_opt=cv2.filter2D(gray,-1,sobel_filter_vertical)\n",
    "    sharp_image_opt=cv2.filter2D(sharp_image_opt,-1,sobel_filter_vertical)\n",
    "    laplacian_filter = np.array([[0, 1, 0],\n",
    "                                 [1, -4, 1],\n",
    "                                 [0, 1, 0]])\n",
    "    laplacian_filter_strong = np.array([[-1, -1, -1],\n",
    "                                 [-1, 8, -1],\n",
    "                                 [-1, -1, -1]])\n",
    "    hbf = np.array([[0, -1, 0],\n",
    "                    [-1, 5, -1],\n",
    "                    [0, -1, 0]])\n",
    "    shbf = np.array([[-1, -1, -1],\n",
    "                    [-1, 9, -1],\n",
    "                    [-1, -1, -1]])\n",
    "    prewitt_horizontal = np.array([[-1, 0, 1],\n",
    "                                   [-1, 0, 1],\n",
    "                                   [-1, 0, 1]])\n",
    "    prewitt_vertical = np.array([[-1, -1, -1],\n",
    "                                [0,  0,  0],\n",
    "                                [1,  1,  1]])\n",
    "    scharr =np.array([[-3, 0, 3],\n",
    "                      [-10, 0, 10],\n",
    "                      [-3, 0, 3]])\n",
    "    if Canny:\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, sobel_filter_horizontal)\n",
    "        sharp_image_opt = cv2.filter2D(sharp_image_opt, -1, sobel_filter_vertical)\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, prewitt_horizontal)\n",
    "        sharp_image_opt = cv2.filter2D(sharp_image_opt, -1, prewitt_vertical)\n",
    "        sharp_image_opt = cv2.filter2D(sharp_image_opt, -1, laplacian_filter_strong)\n",
    "        sharp_image_opt = Image.fromarray(sharp_image_opt)\n",
    "        enhancer = ImageEnhance.Contrast(sharp_image_opt)\n",
    "        factor = 3\n",
    "        sharp_image_opt = enhancer.enhance(factor)\n",
    "        sharp_image_opt = asarray(sharp_image_opt)\n",
    "        sharp_image_opt = cv2.Canny(gray, 50, 100)\n",
    "\n",
    "    else:\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, prewitt_horizontal)\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, prewitt_vertical)\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, laplacian_filter_strong)\n",
    "        sharp_image_opt = Image.fromarray(sharp_image_opt)\n",
    "        enhancer = ImageEnhance.Contrast(sharp_image_opt)\n",
    "        factor = 3\n",
    "        sharp_image_opt = enhancer.enhance(factor)\n",
    "        sharp_image_opt = asarray(sharp_image_opt)\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, scharr)\n",
    "        sharp_image_opt = cv2.filter2D(gray, -1, shbf)\n",
    "\n",
    "    cv2.imshow('Edge Detection', sharp_image_opt)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc6710",
   "metadata": {},
   "source": [
    "# Computing Region of Interest\n",
    "This function takes an image as input and applies a rectangular bounding box to a region of interest (ROI). It can be done either automatically or manually through user input.\n",
    "\n",
    "If automated, the code first converts the image to grayscale and applies the Canny edge detection algorithm to create a sharp image. It then finds the minimum and maximum values in the image and uses them to calculate the starting and ending points of the bounding box for the ROI. The bounding box is drawn on the original image and displayed.\n",
    "\n",
    "If done manually, the code displays the original image and allows the user to draw a rectangular ROI using their mouse. Once the ROI is selected, the code crops the image to only include the selected region and displays the cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392a98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROI_MSER(imagePath, automated):\n",
    "    img = cv2.imread(imagePath)\n",
    "    \n",
    "    if automated:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        sharp_image_opt = cv2.Canny(gray, 50, 100)\n",
    "        (minvalue, maxvalue, minLoc, maxLoc) = cv2.minMaxLoc(sharp_image_opt)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        channels = img.shape[2]\n",
    "\n",
    "        startPointMax = ((maxLoc[0] + minLoc[0]) / 2, (maxLoc[1] + minLoc[1]) / 2)\n",
    "        startPointMax2 = ((maxLoc[0] / 2 + minLoc[0]) / 2, (maxLoc[1] / 2 + minLoc[1]) / 2)\n",
    "        startPointMin = ((minLoc[0] * maxLoc[0]) / 2, (minLoc[1] * maxLoc[1]) / 2)\n",
    "        endPointMax = (startPointMax[0] + width/3, startPointMax[1] + height)\n",
    "        endPointMax2 = (startPointMax2[0] + width/2, startPointMax2[1] + height)\n",
    "        endPointMin = (startPointMin[0] + width/3, startPointMin[1] + height)\n",
    "\n",
    "        colourMax = (0, 0, 0)\n",
    "        colourMax2 = (0, 0, 255)\n",
    "        colourMin = (255, 0, 0)\n",
    "\n",
    "        blueRect = cv2.rectangle(img, (int(startPointMin[0]), int(startPointMin[1])),\n",
    "                                 (int(endPointMin[0]), int(endPointMin[1])), colourMin, 2)\n",
    "        blueArea = (int(startPointMin[0]), int(startPointMin[1]), int(endPointMin[0]), int(endPointMin[1]))\n",
    "\n",
    "        blackRect = cv2.rectangle(img, (int(startPointMax[0]), int(startPointMax[1])),\n",
    "                                  (int(endPointMax[0]), int(endPointMax[1])), colourMax, 2)\n",
    "        redRect = cv2.rectangle(img, (int(startPointMax2[0]), int(startPointMax2[1])),\n",
    "                                (int(endPointMax2[0]), int(endPointMax2[1])), colourMax2, 2)\n",
    "        cv2.imshow(\"Cropped to approximate ROI\", img)\n",
    "    else:\n",
    "        fromCenter = False\n",
    "        rectangles = cv2.selectROI(\"Region Bounding Box\", img, fromCenter)\n",
    "        crop = img[int(rectangles[1]):int(rectangles[1]+rectangles[3]), int(rectangles[0]):int(rectangles[0]+rectangles[2])]\n",
    "        cv2.imshow(\"ROI Image\", crop)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40eba74",
   "metadata": {},
   "source": [
    "# Corner Detection\n",
    "This function takes an image file path as input, reads the image, and then converts it to grayscale. It then applies the Canny edge detection algorithm to the grayscale image. Next, it applies the cornerHarris algorithm to the Canny edge-detected image. Finally, it overlays the original image with the detected corners in red and displays the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d70f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corners(imagePath):\n",
    "    img = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sharp_image_opt = cv2.Canny(gray, 50, 100)\n",
    "    gray = np.float32(sharp_image_opt)\n",
    "    dst = cv2.cornerHarris(sharp_image_opt, 2, 3, 0.04)\n",
    "    img[dst > 0.01 * dst.max()] = [0, 0, 255]\n",
    "    cv2.imshow('Corners', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234de9cc",
   "metadata": {},
   "source": [
    "# Finding an Encompassing Rectangle\n",
    "The function takes in a list of rectangles (in the form of tuples with 4 values: x, y, width, height) and returns the coordinates and dimensions of a rectangle that can encompass all the input rectangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481e0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_encompassing_rect(rect_list):\n",
    "    min_x = float('inf')\n",
    "    min_y = float('inf')\n",
    "    max_x = -float('inf')\n",
    "    max_y = -float('inf')\n",
    "    max_w = -float('inf')\n",
    "    max_h = -float('inf')\n",
    "\n",
    "    for rect in rect_list:\n",
    "        (x, y, w, h) = rect\n",
    "        min_x = min(min_x, x)\n",
    "        min_y = min(min_y, y)\n",
    "        max_x = max(max_x, x + w)\n",
    "        max_y = max(max_y, y + h)\n",
    "        max_w = max(max_w, w)\n",
    "        max_h = max(max_h, h)\n",
    "\n",
    "    return min_x, min_y, max_x - min_x, max_y - min_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade76be",
   "metadata": {},
   "source": [
    "# Cropping to Region of Interest\n",
    "The function takes in an image and finds its contours with Binary and Inverse Binary Thresholding then Finds and encompassing rectangle around then and crops the image to the size of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e93bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_To_ROI(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    digit_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    digit_boxes.extend([cv2.boundingRect(contour) for contour in contours])\n",
    "    (x, y, w, h) = find_encompassing_rect(digit_boxes)\n",
    "    if w > 0 and h > 0:\n",
    "        crop = img[y:y + h, x:x + w]\n",
    "        cv2.imshow(\"Cropped Image\", crop)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd1587",
   "metadata": {},
   "source": [
    "# Loading DataSet\n",
    "The function takes in a file path to a dataset in JSON format and uses the json library to load it into a variable called data it then returns the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112a4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(file_path: str):\n",
    "    f = open(file_path, 'r')\n",
    "    data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86064f",
   "metadata": {},
   "source": [
    "# Accuracy Mode\n",
    "By using harsh accuracy if we get extra bounding contours that aren't around the images we decrease accuracy\n",
    "By not using harsh accuaracy we only compute the intersection over the real contours and discard the extra contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cedd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "HarshAccuracy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84396d",
   "metadata": {},
   "source": [
    "# Getting Percentage of Intersection\n",
    "The function takes in two lists of rectangles (in the form of tuples with 4 values: x, y, width, height) the function then draws each list on top of a black image loaded via cv2.imread('black.png', cv2.IMREAD_GRAYSCALE), The function then performs bitwise anding between them and finds the percentage of the intersection relative to the percentage of white pixels in both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e789e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntersectionPercentage(myOutput, realOutput):\n",
    "    global HarshAccuracy\n",
    "    img1Temp = cv2.imread('black.png', cv2.IMREAD_GRAYSCALE)\n",
    "    img1 = copy.deepcopy(img1Temp)\n",
    "    img2 = cv2.imread('black.png', cv2.IMREAD_GRAYSCALE)\n",
    "    allPercents = []\n",
    "\n",
    "    for (x, y, w, h) in realOutput:\n",
    "        cv2.rectangle(img1, (x, y), (x + w, y + h), 255, 2)\n",
    "\n",
    "    for (x, y, w, h) in myOutput:\n",
    "        cv2.rectangle(img2, (x, y), (x + w, y + h), 255, 3)\n",
    "\n",
    "    interSection = cv2.bitwise_and(img1, img2)\n",
    "    allPercents.append((np.sum(interSection == 255) /\n",
    "                        (np.sum(img1 == 255) + np.sum(img2 == 255) - np.sum(interSection == 255))) * 100)\n",
    "\n",
    "    for shift in [[5, 0], [-5, 0], [0, 5], [0, -5]]:\n",
    "        img1 = copy.deepcopy(img1Temp)\n",
    "        for (x, y, w, h) in realOutput:\n",
    "            cv2.rectangle(img1, (x + shift[0], y + shift[1]), (x + w + shift[0], y + h + shift[1]), 255, 2)\n",
    "        interSection = cv2.bitwise_and(img1, img2)\n",
    "        if HarshAccuracy:\n",
    "            allPercents.append((np.sum(interSection == 255) /\n",
    "                                (np.sum(img1 == 255) + np.sum(img2 == 255) - np.sum(interSection == 255))) * 100)\n",
    "        else:\n",
    "            allPercents.append((np.sum(interSection == 255) /\n",
    "                                (np.sum(img1 == 255))) * 100)\n",
    "\n",
    "    return max(allPercents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b316e2",
   "metadata": {},
   "source": [
    "# Canny Edge Detection\n",
    "First thing the function does os filter the image using a bilateral Filter the function then converts the filtered image to grayscale and applies the Canny algorithm to detect edges finally the function returns the image with canny edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5b126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CannyEdge(img, showSteps=False):\n",
    "\n",
    "    # dst = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "    dst = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "    if showSteps:\n",
    "        cv2.imshow('Noise Reduction', dst)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if showSteps:\n",
    "        cv2.imshow('GrayScale', gray)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    thresh = cv2.Canny(gray, 50, 100)\n",
    "\n",
    "    if showSteps:\n",
    "        cv2.imshow('Canny', thresh)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd9043",
   "metadata": {},
   "source": [
    "# Estimating Digit Area\n",
    "This function estimates the minimum and maximum areas of a digit contour in an input image based on the size of the image. The maximum digit height is assumed to be 80% of the image height, and the maximum digit width is assumed to be 90% of the image width adjusted by the maximum aspect ratio of digits 0-9. Similarly, the minimum digit height and width are assumed to be 10% of the image height and width, respectively, adjusted by the minimum aspect ratio of digits 0-9. These estimates are used to calculate the approximate minimum and maximum areas of a digit contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4181bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estimate_digit_area(image_size):\n",
    "    # Estimate the maximum and minimum sizes of the digits based on the image size\n",
    "    max_digit_height = int(image_size[0] * 0.8)  # assume maximum digit height is 80% of the image height\n",
    "    aspect_ratio = [0.38, 0.51, 0.54, 0.53, 0.55, 0.58, 0.53, 0.47, 0.57, 0.52]  # aspect ratio of digits 0-9\n",
    "    # Assume maximum digit width is 90% of the image width, adjusted by the maximum aspect ratio\n",
    "    max_digit_width = int(image_size[1] * 0.9 * max(aspect_ratio))\n",
    "    # Assume minimum digit height is 10% of the image height\n",
    "    min_digit_height = int(image_size[0] * 0.1)\n",
    "    # Assume minimum digit width is 10% of the image width, adjusted by the minimum aspect ratio\n",
    "    min_digit_width = int(image_size[1] * 0.1 * min(aspect_ratio))\n",
    "\n",
    "    # Calculate the approximate maximum and minimum area of the digit contours based on the estimated sizes\n",
    "    max_digit_area = (max_digit_height * max_digit_width)\n",
    "    min_digit_area = (min_digit_height * min_digit_width)\n",
    "\n",
    "    return min_digit_area, max_digit_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cc117",
   "metadata": {},
   "source": [
    "# Localizing The Digits\n",
    "\n",
    "The function LocalizeDigits takes a grayscale image containing digits as input and performs the following operations to localize the digits:\n",
    "   1) Find the contours in the image using the cv2.findContours() function.\n",
    "   2) Estimate the minimum and maximum areas of the digit contours based on the image size using the Estimate_digit_area()\n",
    "       function.\n",
    "   3) Iterate through each contour found in step 1 and check if its area falls within the estimated range of digit areas.\n",
    "   4) If the area of the contour is within the estimated range, add the bounding box of the contour to the finalContours list.\n",
    "   5) Return the finalContours list containing the bounding boxes of the digit contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf12e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LocalizeDigits(img):\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    minArea, maxArea = Estimate_digit_area(img.shape)\n",
    "    finalContours = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if minArea < area < maxArea:\n",
    "            finalContours.append(cv2.boundingRect(contour))\n",
    "\n",
    "    return finalContours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5dc66a",
   "metadata": {},
   "source": [
    "# Localize Images and Compute Accuracy \n",
    "The Function iterates through all images in the testImages folder, applies the CannyEdge and LocalizeDigits functions to each image, compares the resulting bounding boxes to the ground truth values, and calculates the intersection over union (IoU) percentage for each image. Finally, the function returns a list of IoU percentages for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bb2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LocalizeDir(dataset, showSteps):\n",
    "    percents = []\n",
    "    i = 0\n",
    "    total = len(os.listdir('testImages'))\n",
    "    for filename in os.listdir(\"testImages\"):\n",
    "        i += 1\n",
    "        loadPercent = (i/total)*100\n",
    "        sys.stdout.write(f\"\\rLoading: [{'=' * math.floor(loadPercent/10)}{' ' * (10 - math.floor(loadPercent/10))}] \"\n",
    "                         f\"{round(loadPercent, 1)}%\")\n",
    "        f = os.path.join(\"testImages\", filename)\n",
    "        imgReal = cv2.imread(f)\n",
    "\n",
    "        myOutput = LocalizeDigits(\n",
    "            CannyEdge(imgReal, showSteps=showSteps))\n",
    "        realOutput = []\n",
    "\n",
    "        for box in dataset[int(filename.split(\".\")[0]) - 1]['boxes']:\n",
    "            realOutput.append((int(box['left']), int(box['top']), int(box['width']), int(box['height'])))\n",
    "\n",
    "        percent = getIntersectionPercentage(myOutput, realOutput)\n",
    "        percents.append(percent)\n",
    "\n",
    "        if showSteps:\n",
    "            for (x, y, w, h) in myOutput:\n",
    "                cv2.rectangle(imgReal, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('Localized', imgReal)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    return percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8851a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: [==========] 100.0%\n",
      "\n",
      "Accuracy is: 38.2%\n"
     ]
    }
   ],
   "source": [
    "dataSet = loadDataSet('training.json')\n",
    "percentages = LocalizeDir(dataSet, showSteps=False)\n",
    "print(f\"\\n\\nAccuracy is: {round(sum(percentages)/len(percentages), 1)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
